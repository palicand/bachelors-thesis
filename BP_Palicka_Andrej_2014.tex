% options:
% thesis=B bachelor's thesis
% thesis=M master's thesis
% czech thesis in Czech language
% english thesis in English language
% hidelinks remove colour boxes around hyperlinks

\documentclass[thesis=B,english]{FITthesis}[2012/10/20]

\usepackage[utf8]{inputenc} % LaTeX source encoded as UTF-8
% \usepackage[latin2]{inputenc} % LaTeX source encoded as ISO-8859-2
% \usepackage[cp1250]{inputenc} % LaTeX source encoded as Windows-1250

\usepackage{graphicx} %graphics files inclusion
% \usepackage{subfig} %subfigures
\usepackage{amsmath} %advanced maths
\usepackage{amssymb} %additional math symbols
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{dirtree} %directory tree visualisation
% % list of acronyms
\usepackage[acronym,nonumberlist,toc,numberedsection=autolabel]{glossaries}
% \iflanguage{czech}{\renewcommand*{\acronymname}{Seznam pou{\v z}it{\' y}ch zkratek}}{}
\makeglossaries

% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % 
% EDIT THIS
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % 

\department{Department of Computer Science}
\title{Application of Random Decision Forests in Astroinformatics}
\authorGN{Andrej} %author's given name/names
\authorFN{Pali{\v c}ka} %author's surname
\author{Andrej Pali{\v c}ka} %author's name without academic degrees
\authorWithDegrees{Andrej Pali{\v c}ka} %author's name with academic degrees
\supervisor{RNDr. Petr {\v S}koda, CSc.}
\acknowledgements{THANKS}
\abstractEN{Summarize the contents and contribution of your work in a few sentences in English language.}
\abstractCS{V n{\v e}kolika v{\v e}t{\' a}ch shr{\v n}te obsah a p{\v r}{\' i}nos t{\' e}to pr{\' a}ce v {\v c}esk{\' e}m jazyce.}
\placeForDeclarationOfAuthenticity{Prague}
\keywordsCS{rozhodovacie stromy, rozhodovacie lesy, data mining, astroinformatika}
\keywordsEN{decsion trees, decision forests, data mining, astroinformatics}
\declarationOfAuthenticityOption{1} %select as appropriate, according to the desired license


\begin{document}

% \newacronym{CVUT}{{\v C}VUT}{{\v C}esk{\' e} vysok{\' e} u{\v c}en{\' i} technick{\' e} v Praze}
% \newacronym{FIT}{FIT}{Fakulta informa{\v c}n{\' i}ch technologi{\' i}}

\setsecnumdepth{part}
\chapter{Introduction}
Machine learning has become phenomenon in recent years. Many businesses, institutes or organizations are increasingly more dependent on getting knowledge from data they managed to gather through their endeavors. One field that could highly benefit from machine learning approach is astroinformatics.  Astronomical surveys generate high dimensional data with millions of records. However, conventional data mining algorithms fail on these kinds of data both in accuracy and in performance. 

Random decision forests \cite{BR01} is an ensemble data mining model capable of effectively perform classification or regression on such data. It consists of K decision trees, each built from a random sample of the data set. Each node is built from a random subset of features, which is significantly smaller for high dimensional data than the full set. Although RDF generally performs very well in terms of accuracy, the computational cost of building large amounts of trees is very high. To succesfully apply the RDF to data such as astronomical spectras, it needs to be scalable and distributed amongst numerous computational nodes.

I introduce common data mining concepts and techniques in chapter~\ref{chap:DM}. Chapter~\ref{chap:DT} discusses the basic block of the RDF: the decission trees. Since some distributed RDF algorithms require parallel tree building process I also examine scallable approaches to induction of decision trees. Chapter~\ref{chap:RDF} defines the RDF, examines some of their properties, and provides examples of both serial and parallel implementations of the algorithm. I provide a survey of applications of RDF on data from astronomy, medicine, chemistry and others in chapter~\ref{chap:Applications}. In chapter~\ref{chap:Experiments} I provide results from my own experiments with some data sets from the UCI repository and on data sets from the Ondrejov observatory and others. 
\setsecnumdepth{all}
\chapter{Data mining}
\label{chap:DM}
Data mining is a method 
\chapter{Decision trees}
\label{chap:DT}
		Decision trees are a group of supervised data mining methods, that use a tree-like structure to analyze data. They can perform either a classification of data, or a regression of data. The inner nodes of a decision tree represent testing on input attributes. \cite{CMP07} Terminal nodes in a classification tree represent the possible classes of tested objects. In a regression tree they represent predicted values for a given branch. Terminal nodes can either contain single values, or a probability vector of values \cite{TOP_DOWN_INDUCTION_SURVEY}. 

		The process of growing the decision tree is called an induction. Induction of an ideal decision tree is known to be NP-complete \cite{NP-COMPLETE}, so we have to use a heuristical approach for real-world applications. The induction consists of selecting the training data set, determining the splits at each nodes, choosing a stopping criterion and pruning of the tree. The methods we choose to perform these processes with, directly affect the performance of the tree. 

		Traditionally \cite{CART}, the induction of a DT is performed in a recursive fashion, which is fine for small data, but tends to fail on big data problems.

		\section{Choosing the sample}
			There are several approaches to choosing the training and testing sample. The simplest method is to just choose a fraction of the data as a training set and leave the rest for testing the tree. 
		
		\section{Splitting the nodes}
			Splitting is generally the costliest and most important phase of the induction. For each internal node, the inductor examines the input attributes in attempt to find the best split, i.e. at least one attribute, upon which it would branch the node. 

			An attribute may be either categorical (discrete) or numerical (continuous). Evaluating categorical attributes is generally straightforward, the easiest solution is to just create a branch for each possible value. A more complex one is to group several values together and creating a branch for each. 

			Additional steps must be taken when dealing with numerical attributes \cite{C45-NUMERICAL}.	 In this case the node is branched based on some threshold value or values which we need to find. At first, the data must be sorted based on the attribute. We'll mark those sorted values as \(\{v_1..v_m\}\). Each value \(v_i\) is then used as a dicrete value by the splitting criteria to determine the best split between two values \(v_i\) and \(v_{i+1}\).

			To determine how to split the nodes in the tree, we need to evaluate splitting criteria. There are essentially two types of splitting criteria:
			\begin{itemize}
			\item Univariate
			\item Multivariate
			\end{itemize}
			The univariate splitting criteria split the node according to the value of one attribute, while the multivariate criteria split the node on multiple attributes.

			\subsection{Univariate criteria}
				There are several categories of univariate criteria, such as impurity-based, normalized impurity-based or binary criteria. \cite{DMWithDecisionTrees}
				\subsubsection{Impurity-based criteria}
				Impurity-based criteria use an impurity measure, which is a function \(\phi\lbrack0,1\rbrack^k->R\). It for input \(P\), \(\phi(P)\) satisfies the following conditions:
				\begin{enumerate}
				\item \(\phi(P) \geq 0\)
				\item It's minimum if \(\exists i\) such that \(p_i = 1\)
				\item It's maximum if \(\forall i, 1 \leq i \leq k, p_i = 1/k\)
				\item It's symmetric with respect to components of P.
				\item It's smooth (differentiable everywhere) in its range.	
				\end{enumerate}
				The goodness-of-split due to discrete attribute \(a_i\) is defined as reduction in impurity of the target attribute after partitioning \(S\) according to the values \(v_{i,j} \in \textit{dom}(a_i)\):
				\[
					\Delta\phi(a_i,S)=\phi(P_y(S))-\sum\limits_{j=1}^{|\textit{dom}(a_i)|}{\frac{|\sigma_{a_i}=v_{i,j}S|}{|S|}*\phi\left(P_y\left(\sigma_{a_i}=v_{i,j}S\right)\right)}
				\]

				\paragraph*{Information Gain}
				Information gain is an  impurity-based selection. It uses an entropy of the input attributes as the impurity measure. The entropy of a set D is defined as:
				\[
				\textit{INSERT DEFINITION HERE!!}
				\] 
				Because of the attributes of the entropy, it tends to favor attributes with a lot of values. This can turn out to be problematic, if e.g. we'd include attributes that are unique for each specimen, such as an ID. \cite{ENSEMBLE}
				
				This can be remedied by using a gain ratio instead. It normalizes the information gain against the number of values of the feature.
				\paragraph*{Gini index}
				The Gini index is an impurity-based criteria that measures the divergences between the probability distributions of the target attributes values \cite{DMWithDecisionTrees}. It was popularized by the CART algorithm \cite{CART}. It is defined as follows:
				\[
					\textit{Gini}\left(y, S\right)=1 - \sum _{c_j \in \textit{dom(y)}}\left(\frac{|\sigma_{y=c_j}S|}{|S|}\right)^2
				\]
				\subsubsection{Binary criteria}
				Binary criteria are used to create binary trees. Binary criteria divide the domain of the input attribute into two subdomains. Let \(\beta(a_i, d_1, d_2, S)\) be a binary criterion value for attribute \(a_i\) over the data set \(S\) that divides it into two mutually exclusive and exhaustive subdomains \(d_1\) and \(d_2\). The binary criterion attemps to find the maximum value for the split, while satisfying the conditions for \(d_1\) and \(d_2\). An example of binary criterion is a twoing criterion. It was suggested in \cite{CART} after discovering that the Gini index performs badly when encountering wide domains. The twoing criterion is defined as
				\begin{multline}
					\textit{twoing}(a_i, S, d_1, d_2) = 0.25 * \frac{|\sigma_{a_{i} \in d_1}S|}{|S|}* \frac{|\sigma_{a_{i} \in d_2}S|}{|S|}  * \\ \left(\sum_{c_{1}\in\textit{dom}(y)}{
					\left|
					\frac{\left|
							\sigma_{a_{i}\in d_{1} \wedge y=c_i}S
						\right|}
						{|\sigma_{a_{i} \in d_1}S|} - \frac{\left|
							\sigma_{a_{i}\in d_{2} \wedge y=c_i}S
						\right|}
						{|\sigma_{a_{i} \in d_2}S|}
					\right|
					}\right)^2
				\end{multline}
		\subsection{Multivariate criteria}
			Although multivariate criteria are more effective than univariate, they are much more difficult to implement. They are usually based around finding the best linear combination of the attributes. Methods for finding this combination include:
			\begin{itemize}
				\item Neural trees \cite{NTrees}
				\item Hill climbing \cite{CART}
				\item Linear programming
			\end{itemize}

		\section{Stopping criteria and pruning}
			Stopping criteria tell the inducer when to stop splitting the nodes. Upon fullfilling the stopping criteria, the node is assigned the output label instead of splitting. There are various conditions and their combinations that can be used as the criteria, such as reaching the maximum tree depth, not gaining any significant information from the splitting criteria or not having enough cases in nodes. Choosing right stopping criteria is vital for the tree, because choosing very strict criteria may lead to an underfit tree. Making the criteria too benevolent, however, leads to overfitting the tree and losing information about the relationship between various attributes. To provide a solution, \cite{CART} introduced a pruning algorithm, that allows the tree to overfit and then prunes it to remove branches that cause the tree to lose generality.
				
		\subsection{Pruning}
		\label{sec:pruning}
				\paragraph*{Cost-complexity pruning}

				Pruning method suggested by \cite{CART}. The pruning is executed in two phases. In the first phase, we produce a sequence of trees \(T_0, T_1, T_2, \dots, T_k \), were \(T_0\) is the initial unpruned tree obtained by the induction and \(T_k\) is a tree consisting only of the root node. Each tree is obtained by replacing one or more subtrees in its predecessor by leaves. To determine which subtree to remove, we calculate an increase in error rate per pruned leaf by 
				\[\alpha = \frac{\mathrm{err(pruned(}T,t\mathrm{)},S\mathrm{)} - \mathrm{err(}T,S\mathrm{)}}{|\mathrm{leaves(}T\mathrm{)}|-|\mathrm{leaves(}\mathrm{err(pruned(}T,t\mathrm{)}\mathrm{)}|}\]
				where \emph{err} is an error rate for given tree \(T\) and set \(S\), \emph{pruned} is a pruned tree from \(T\) without subtree \(t\) and \emph{leaves} is a number of leaves in \(T\).

				In the second phase, we compute the generalization error for each tree \(T_i\), and select the tree with the lowest error. 

				\paragraph*{Reduced-Error Pruning}

				This method was introduced by \cite{Quinlan1987221}. We perform a test run on the original tree and check for each non-leaf node, whether replacing it with the most common class would reduce the error rate. This process continues, until we cannot reduce the error rate any more.

		\section{Testing the data}
		Once the tree induction process is complete, we can begin with testing the unknown data. Beginning at the root node, each entry from the data is tested based on the attributes the node represents and then passed into the subtree, where the process repeats itself. Once it reaches the leaf node, it's assigned the class that is in the node.
		
		\section{Decision trees algorithms}
			\paragraph*{ID3} It is a very simple decision tree algorithm proposed by Quinlan in \cite{INDUCTIONOFDT}. It uses information gain as a splitting criterion. The induction stops either when all the data belong to a single value, or when the information gain is less or equal to zero. The algorithm fails on missing attributes and cannot handle numerical attributes. It does not perform pruning.

			\paragraph*{C4.5} An improved version of the ID3 algorithm \cite{C45-NUMERICAL} that solves many of its problems. Instead of the information gain, it uses a gain ratio. The algorithm performs error-based pruning, handles missing values through corrected gain ratio criteria and can induce from data sets that contain numeric attributes. The inducing stops when the number of instances to be split is below certain threshold. Both ID3 and C4.5 can't handle target labels that contain continuous values.

			\paragraph*{CART} Introduced in \cite{CART}. CART produces binary trees. What makes this algorithm significant and useful is that it handles both continuous and discrete output features and thus is capable of both classification and regression. It uses twoing criteria for classification and cost-complexity pruning. For regression it tries to minimize the prediciton squared error. Users can enhance it's accuracy by providing probability distribution \textit{a priori}, and it can consider missclassification cost.

		\section{Performance and scalability}
		Traditional approaches to induction do not scale well for big data. Since they are usually built in a recursive fashion and require the data to be in the memory all the time, the system may run out of memory \cite{SCALABLE_RDF}. 

		To overcome these limitations, we can parallelize the induction process, distributing it across several computing nodes. While this makes an implementation of the DT much more complicated, it allows for huge data sets, thus improving the accuracy of the tree.

		\subsection{Types of parallelism}
		There are several strategies for parallelization of DT using task parallelism, data parallelism or hybrid prallelism. \cite{PARALLEL_IMPLEMENTATION}
		\paragraph*{Task parallelism.} When using task parallelism, we distribute the decision nodes across several processors. The induction process starts on a single processor, which then proceeds with the induction as in the serial approach. When the number of decision nodes reaches the number of available processors, they are distributed among them. Then, each processor continues in the induction of the subtree with the assigned node as its root. The subtrees are then merged together as they are completed. An example of this approach is a parallel implementation of C4.5 algorithm in \cite{PARALLEL_INDUCTION}.

		This approach has its disadvantages. The load on each processor is imbalanced, because each subtree can take considerabely different time to construct. This could be countered by splitting the subtrees further, after some processor finished its task. Moreover, each processor needs to have access to the full data set, meaning it either has to be copied to each processor, or the processors have to access it from a central location, creating a great amount of communication. 

		\paragraph*{Data parallelism.} In data parallelism, the data, instead of the tree nodes, are split among the processors. The processors construct each node together, locally evaluating the data they were assigned. Because the processors have to communicate and share their split evaluations each time a new node is created the data parallelism generates a lot of communication on the network. The data can either be split vertically or horizontally. 

		In a vertical split, each processor receives every record and its label, but only a distinct set of input attributes. When deciding where to split, it takes into account only the set of attributes it received. Each processor supplies the best split from its local attributes. From these local splits the best one is picked. The vertical split strategy can still suffer from load imbalance, because continuous attributes usually take longer to evaluate.

		When using a horizontal split, each processor is assigned a distinct set of the records, but with full set of attributes. Each processor then examines possible splits based on the data it received and then communicates its findings with the other processors. They then work together to find the best global split.

		\paragraph*{Hybrid parallelism.} This is a combination of the data and task parallelism. It tries to combine the low communication overhead of the task parallelism with the lower memory requirements and processing amount of the data parallelism. The nodes that would have to cover large amount of examples use data parallelism to lower the load balance of each node. On the other hand, nodes, that cover few examples can be effectively distributed among several processors without significant memory overhead and thus eliminating costly interprocessor communication.

		\subsection{Examples of parallel DT algorithms}
			\paragraph*{SLIQ}
			\paragraph*{title}
			\paragraph*{SPDT} 
		\section{Advantages and disadvantages}
		As we can see, one of the main advantages of a DT is its simplicity and intuitiveness. They can be used on all sorts of data, both for classification and for regression. They allow for errors in data and, with some improvements, can handle missing values well \cite{DMWithDecisionTrees,CMP07}. Decision trees can be converted to a set of rules just by following branches from the root node to a leaf node. Some are capable of handling both numeric and categorical values and some algorithms can be used both for classification and regression.

		On the other hand, non-parallel decision trees do not perform well on high dimensional data. They are also sensitive to noise and irrelevant attributes and they easily overfit or underfit.
\chapter{Random decision forests}
\label{chap:RDF}
	RDF is an ensemble classification method using \(K\) decision tree classifiers \({h(x, \Phi_k), k = 1\dots K} \), where \(x\) is the input data and \({\Phi_k}\) are independent identically distributed random vectors \cite{SELECTION_OF_DT}. The random vector \(\Phi_k\) represents the randomness that is injected into each DT and it's nature and content is dependent on a concrete use. The randomness is usually introduced by randomly sampling the subset of input features and/or by bagging. After finishing the run in each tree, they vote for the final result, which will be chosen as the final class that will be assigned to that data entry.

	As with the Decision Trees, the performance of the RDF is directly affected by several factors such as the number of the trees, the means of introducing randomness into the induction, the voting process or the induction of the trees themselves.



	\section{Number of trees}
	It can be shown, that by adding more trees, the error rate does not increase or decrease infinitely, but converges to a limiting value. Suppose we have an ensemble of classifiers \(h_1(\textbf{x}), h_2(\textbf{x}),\dots,h_K(\textbf{x})\). Define function \(\textit{mg}(\mathbf{X},Y)\) as:
	\[\textit{mg}(\mathbf{X},Y)=\textit{avg}_kI(h_k(\mathbf{X})=Y)-\textit{max}I(h_k(\mathbf{X})=j)\] It gives an amount by which an average number of votes for the right class exceedes the average number of votes for any other class. 

	Define generalization error as \(PE^*=P_{\mathbf{X},Y}(\textit{mg}(\mathbf{X},Y) < 0)\). In decision forests, the above mentioned classifier \(h_k(\textbf{X})=h_k(X, \Phi_k)\). By applying the Strong Law of Large Numbers, we can conclude, that by increasing the number of trees, the generalization error \(PE^*\) of all sequences~of~\(h_1(\mathbf{X},\Phi_k)\), \(h_2(\mathbf{X}, \Phi_k)\),~\(\dots, h_K(\mathbf{X}, \Phi_k)\) almost surely converge to \cite{BR01}:
	\[
	P_{\mathbf{X},Y}(P_{\Phi}(h(\mathbf{X}, \Phi)=Y)-\textit{max}P_{\Phi}(h(\mathbf{X}, \Phi)=j))
	\]
	This implies that there is a threshold for the number of trees in the forest, above which the trees provide only small or nonexistent improvement to the accuracy \cite{SELECTION_OF_DT}. This of course does not mean that the forest has reached an optimal number of trees --- the optimum is a subset of the built trees. To reduce the number of trees, we can prune the forest, not unlike we did with individual trees as described in section~\ref{sec:pruning} --- in RDF, we call this process selection. To do this, we need to define selection criteria and selection method.

	Selection criteria divide into filters and wrappers. Filtering criteria select trees \textit{a priori}, not taking into account the combined accuracy of the subset. Wrapper criteria select trees \textit{a posteriori}, by optimizing the combined performance of the subset \cite{PRUNING_RDF}. Wrapper criteria are generally better for building optimized subset of classifiers out of an existing set. Methods of selection may be based on several approaches:
	\paragraph*{Heuristic rules.} These methods usually use simple rules based upon some metric of the classifiers. An example of these may be 
	\paragraph*{Diversity measures.} Lorem ipsum dolor sit amet, consectetur adipiscing elit. In vel nisl id quam venenatis varius vel ut leo. Cras bibendum commodo ipsum non dictum. Quisque libero nulla, lacinia vel eros eget, rutrum lacinia lorem. Nulla interdum augue eget dictum tempus. Etiam consequat non augue ac ultricies. In euismod, quam sed.
	\paragraph*{Search algorithms.}Lorem ipsum dolor sit amet, consectetur adipiscing elit. In vel nisl id quam venenatis varius vel ut leo. Cras bibendum commodo ipsum non dictum. Quisque libero nulla, lacinia vel eros eget, rutrum lacinia lorem. Nulla interdum augue eget dictum tempus. Etiam consequat non augue ac ultricies. In euismod, quam sed.
	\paragraph*{Classfier clustering.} Lorem ipsum dolor sit amet, consectetur adipiscing elit. In vel nisl id quam venenatis varius vel ut leo. Cras bibendum commodo ipsum non dictum. Quisque libero nulla, lacinia vel eros eget, rutrum lacinia lorem. Nulla interdum augue eget dictum tempus. Etiam consequat non augue ac ultricies. In euismod, quam sed.
	\paragraph*{Evolutionary algorithms.} Lorem ipsum dolor sit amet, consectetur adipiscing elit. In vel nisl id quam venenatis varius vel ut leo. Cras bibendum commodo ipsum non dictum. Quisque libero nulla, lacinia vel eros eget, rutrum lacinia lorem. Nulla interdum augue eget dictum tempus. Etiam consequat non augue ac ultricies. In euismod, quam sed.

	\section{Bagging}
	One of the techniques that is used in the RDF is \textbf{b}ootstrap \textbf{agg}regat\textbf{ing}. For each sampled instance \(T_k\) of the training set \(T\), \(k = 1, 2, \dots, K \) we randomly sample a training set with replacement from the original training set. It has the same size as the original and because it was sampled with replacement, some of the instances may appear more than once, while some may be completely excluded. \cite{quinlan1996bagging}. During the induction the bagging is used in two ways. One is for the induction of the trees, where each tree is induced from a different sample. The second is to give ongoing estimates about the generalization error, strength, correlation or variable importance.

	The most common out-of-bag estimate is the estimate of the generalization error. Suppose we have a classifier \(h_k\) which was trained on the training set \(T_k\) sampled from \(T\). Then for each \(y,\mathbf{x}\) from the training set \(T\) we aggregate those classifiers, that do not contain the given instance and let those classifiers vote for the right results. The out-of-bag error rate is the average of error rates all such aggregate classifiers on the training set \cite{breiman1996out}. To get 

	\section{Parallelization}
	Because the induction of the random forest is based on building of large amount of trees, it is very costly for data with millions of records and many dimensions. That makes the induction of decision forest a viable candidate for attempts on parallel implementation. There are several ways to parallelize the induction.

	One of the possible approaches is to use task parallelism to induce serial trees such as CART or C4.5 and simply distribute the induction of distinct subsets of trees among processors \cite{RED13}. The induction can then 

	Another approach 

	\section{RDF algorithms and implementations}
	The classic RDF algorithm, called Forest-RI was introduced by Breiman~in~\cite{BR01}. It serves as a basis for all the other random forest algorithms. It builds \(n\) trees using a modified CART algorithm, where each node is built using \(m\) randomly selected features. Breiman suggests using \(\lfloor \log_2|A|)+1\rfloor\) as the~number of~features, where \(A\) is the input features vector. Neither the trees nor the forest are pruned.
	\begin{algorithm}[H]
		\caption{FOREST-RI}
		\label{alg:forestri}
		\begin{algorithmic}
			\Require{$S$ is a data set, $A$ is a vector of input attributes, $N$ an integer specifying the number of trees to build and }
			\Function{BuildForest}{$S$, $A$, $N$, $F$ is an integer specifying number of features to choose from in each node}
				\For{$i \gets 1, N$}
					Randomly select $F$ features from $A$

				\EndFor
			\EndFunction
		\end{algorithmic}
	\end{algorithm}
\chapter{Applications of RDF on Big Data}
\label{chap:Applications}
\chapter{Experiments}
\label{chap:Experiments}
\chapter{Conclusion}


\bibliographystyle{iso690}
\bibliography{bibliography}

\setsecnumdepth{all}
\appendix

\chapter{Acronyms}
% \printglossaries
\begin{description}
	\item[RDF] Random decision forest
	\item[DT] Decision tree
	\item[SVM] Support vector machine
\end{description}


\chapter{Contents of enclosed CD}

%change appropriately

\begin{figure}
	\dirtree{%
		.1 readme.txt\DTcomment{the file with CD contents description}.
		.1 exe\DTcomment{the directory with executables}.
		.1 src\DTcomment{the directory of source codes}.
		.2 wbdcm\DTcomment{implementation sources}.
		.2 thesis\DTcomment{the directory of \LaTeX{} source codes of the thesis}.
		.1 text\DTcomment{the thesis text directory}.
		.2 thesis.pdf\DTcomment{the thesis text in PDF format}.
		.2 thesis.ps\DTcomment{the thesis text in PS format}.
	}
\end{figure}

\end{document}
